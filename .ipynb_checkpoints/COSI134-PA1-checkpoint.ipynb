{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COSI 134 PA 1: Naive Bayes Classifier\n",
    "---\n",
    "\n",
    "## Project Description \n",
    "\n",
    "You are to write a Naive Bayes classifier and apply it to a text classification task (predicting the genders of the authors of blog posts). You must implement training and classification methods for your classifier, design and implement features specific to the given corpus, and report on your results. You are to submit a zip or tarball to Latte containing your code, a 1-2 page report (as a PDF file), and a model file for your best-performing classifier (see below).\n",
    "\n",
    "## Background\n",
    "A supervised text classifier such as Naive Bayes uses a trained statistical model to infer or predict a label (also called a ‘class’ or ‘category’) for an unlabeled document (or ‘instance’), given a vector of features. Useful resources:\n",
    "\n",
    "1. Lecture 3 handout\n",
    "2. [Chapter 7 of Speech and Language Processing, Dan Jurafsky and James H. Martin](https://web.stanford.edu/~jurafsky/slp3/)\n",
    "3. [Chapter 6 of NLTK book](http://www.nltk.org/book/ch06.html)\n",
    "4. [Naive Bayes documentation in sklearn](http://scikit-learn.org/stable/modules/naive_bayes.html)\n",
    "\n",
    "> **Note:**\n",
    "The tarball distributed with this assignment includes the raw corpus data and a few Python modules to help you get started. The included code is reasonably well documented (please read it!)  and has been tested, but if you find anything unclear or incorrect, please let us know right away. If you want to just ignore it and start from scratch, that’s fine, but not recommended.\n",
    "\n",
    "## Corpora\n",
    "In this assignment, the documents you are responsible for classifying are blog posts; the labels will be ‘M’ for ‘male’ or ‘F’ for ‘female’, designating the gender of the post’s author. \n",
    "\n",
    "### Blog posts dataset\n",
    "There are 3,232 blog posts in the supplied corpus, which should be enough data to achieve reasonable results (see below for specifics) without taking unduly long to train.\n",
    "\n",
    "### Names Corpus\n",
    "In addition to the blog gender corpus, a corpus of personal names is also provided, each of which is labeled as ‘male’ or ‘female’. There are 5,001 female names and 2,943 male names, making this corpus ‘larger’ than the blog corpus; but each document consists of just one word, which makes dealing with them substantially faster and easier. \n",
    "\n",
    "> **Note:**\n",
    "You may find it helpful to develop your classifier first with the names corpus and then move on to the blog post, but accurately classifying names is not a requirement for this assignment.\n",
    "\n",
    "### API for handling Corpus\n",
    "The stater code contains a *corpus.py* module which serves as an API for handling all kinds of data instances involved in our corpus.\n",
    "\n",
    "Class *Corpus*:\n",
    "This abstract class acts as a list-like container of data instances, for different type of corpus, e.g. Names Corpus, Blog Corpus, we have provided the inherented classes, *NamesCorpus* and *BlogsCorpus* to handle loading the data. You don't have to extend this part throughout the project.\n",
    "\n",
    "Class *Document*:\n",
    "This class represents data instance in the corpus, e.g. in names corpus it represents $(name, label)$ pair; in blogs post corpus it represents $(blog\\_post,label)$ pair.\n",
    "\n",
    "It only provides basic *features* function which just returns the blog post. The following code shows how we load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "from corpus import BlogsCorpus, Document\n",
    "from test_naive_bayes import Name\n",
    "\n",
    "# here each blog is a Document object\n",
    "blogs = BlogsCorpus(document_class=Document)\n",
    "\n",
    "# here each name is a Name object\n",
    "names = BlogsCorpus(document_class=Name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The *NamesCorpus* and *BlogsCorpus* classes are the only ones that you should need to use directly. You will be responsible for writing subclasses of *Document* that include *features* methods tailored to the blog gender corpus, like the examples in the test_naive_bayes module (classes *EvenOdd*, *BagOfWords* and *Name*).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier module contains an abstract base *Classifier* class that you should use as a superclass for your Naive Bayes classifier. An example subclass called *TrivialClassifier* may be found in the *test_classifier* module; it doesn’t do much, but it illustrates the basic interface. You should not need to change anything in either of those two modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your classifier should go in the *naive_bayes* module, which is just a skeleton in the distributed code. Your job is to flesh out that skeleton. The included *test_naive_bayes* module includes some basic tests, including ones for baseline classifier performance; they will obviously fail at first, but should all pass by the time you’re done.\n",
    "\n",
    ">**Note:**\n",
    "You may (but need not) add your own modules to the ones provided. You may also (but need not) use any third-party libraries you wish (e.g., NLTK, NumPy), but you cannot just import (or copy!) a third-party Naive Bayes classifier; its implementation must be entirely your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The tests in the distributed *test_naive_bayes* module check the accuracy (i.e., the proportion of correctly classified documents) for:\n",
    "1. a ‘pseudo-corpus’ of integers trivially classified as ‘even’ or ‘odd’. \n",
    "2. Names gender Corpus \n",
    "3. Blog posts gender Corpus\n",
    "\n",
    "Your model should be able to achieve the following performance given the specified data split and *features* functions:\n",
    "1. 100% for the integers, \n",
    "2. 70% for the names corpus with the supplied features\n",
    "3. 55% for the blogs corpus with bag-of-words as features\n",
    "\n",
    "These are the baselines for your Naive Bayes model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve the model\n",
    "\n",
    "Just implementing a classifier is not sufficient—you have to show that it works, and then make it work better!\n",
    "\n",
    "Once your classifier is working (i.e., it can pass those baseline tests), your next task is to improve its performance on the blog gender corpus. A state-of-the-art classifier (not Naive Bayes) with linguistically sophisticated features can achieve close to 90% accuracy (see the paper [Improving Gender Classification of Blog Authors](https://www.cs.uic.edu/~liub/publications/EMNLP-2010-blog-gender.pdf) by Mukherjee and Liu). However, you are not required to beat this :)\n",
    "\n",
    "Here are some methods you could try to improve your model:\n",
    "\n",
    "- **Feature Engineering.** \n",
    "Feature engineering is the part that needs more about linguistic intuition and involves a lot of trial-and-error. Clearly organize your trained models (you can save it using the classifier's *save* method) and record how each feature set affects your performance.\n",
    "Start with some simple things first: experiment with tokenization, n-grams, frequency counts, etc.\n",
    "\n",
    "\n",
    "- **Different smoothing techniques.** \n",
    "Smoothing gives us a more realistic distribution by allocating estimated probability to the unseen data. Try using different smoothing technique and measure the performance.\n",
    "\n",
    "\n",
    "- **Bernoulli vs Multinomial.**\n",
    "In class, we have discussed the difference between the two variation of the Naive Bayes model. These two methods imply different assumptions about the real data distribution. Try comparing and analyzing the result.\n",
    "\n",
    "### Compare different evaluation metrics\n",
    "Sometimes it's not enough to only have one metric for the task. You should utilize more measurements to get clear understanding of how your model perform.\n",
    "\n",
    "We also included an imbalanced data set (~90% of the data belongs to one class). When improving your model, evaluating the performance with both **accuracy** and **F-measure**, see how different metrics change. Plot and analyze what you have found. \n",
    "\n",
    ">**Note:** you need to implement your own F-measure function. And since F-measure is a per-class measurement in our case. You could use unweighted (averaging) of F-measure score of both classes or use weighted one (weighted by the proportion of each class)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading\n",
    "Your grade will be based on the correctness of your classifier’s training and classification algorithms (60%), its performance on the blog gender corpus with your best set of features (10%), code clarity and style (10%), and the quality of your report (20%)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
